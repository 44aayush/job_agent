# job_agent

A small collection of Python utilities and a GitHub Action that help you find and archive job listings focused on DevSecOps, Cloud Security, Platform Engineering, and related roles in Germany and Remote.

**What this repository contains**

- `job_search_agent.py` — Generates a human-friendly HTML dashboard (`job_search_dashboard.html`) with one-click search links for configured job platforms (LinkedIn, StepStone, Indeed, Xing, Google Jobs, ArbeitNow). It's useful for interactive searching and quick manual applications.
- `job_agent.py` — Programmatic job fetcher that queries the ArbeitNow API and (optionally) SerpApi (via the `SERPAPI_KEY` env var) to collect matching job postings and saves them to `job_results/<DATE>_Result.txt`.
- `.github/workflows/job_search.yml` — GitHub Actions workflow that runs `job_agent.py` daily (and on manual dispatch), installs dependencies and commits any new results to the repository.

**Key ideas**

- Two complementary approaches: an interactive dashboard for manual searches (`job_search_agent.py`) and an automated collector that archives results (`job_agent.py`).
- The GitHub Action runs the automated collector daily and commits new `job_results` files so you can keep a searchable archive in the repo.

**Requirements**

- Python 3.7+
- `requests` (used by `job_agent.py`) — install with:

```bash
pip install requests
```

**Environment variables**

- `SERPAPI_KEY` — optional. If set, `job_agent.py` will call SerpApi to fetch Google Jobs aggregated results (LinkedIn/Xing/Indeed via Google Jobs). If not set, SerpApi-related fetches are skipped.

**Usage**

- Generate the interactive dashboard (creates `job_search_dashboard.html` and opens it in your default browser):

```bash
python3 job_search_agent.py
```

- Run the automated fetcher (saves results into `job_results/`):

```bash
# locally without SerpApi
python3 job_agent.py

# with SerpApi
export SERPAPI_KEY="your_serpapi_key_here"
python3 job_agent.py
```

**GitHub Actions workflow**

- Location: `.github/workflows/job_search.yml`.
- Behavior:
	- Runs daily at midnight UTC (cron `0 0 * * *`) and can be manually dispatched.
	- Installs `requests`, runs `python job_agent.py` with `SERPAPI_KEY` taken from repository secrets.
	- Commits and pushes new files under `job_results/` if the run produced new or changed output.

**Customize**

- To change target roles, edit the keyword lists in either script:
	- `KEYWORDS` in `job_search_agent.py` controls dashboard queries.
	- `BASE_KEYWORDS` in `job_agent.py` controls programmatic matching.
- To add or adjust platforms or URL templates, edit `PLATFORMS` in `job_search_agent.py` (ensure templates include `{query}` and `{location}`).
- For stricter URL encoding (recommended if you add special characters), replace the current simple `replace(' ', '+')` with `urllib.parse.quote_plus`.

**Output locations**

- `job_search_dashboard.html` — generated by running `job_search_agent.py`.
- `job_results/<DDMMYY>_Result.txt` — generated by `job_agent.py`.

**Notes & tips**

- The repository's automated workflow commits `job_results/` back to the repository. Ensure your workflow has write permission and that `SERPAPI_KEY` (if used) is stored in `Settings → Secrets`.
- If running in a headless environment (CI/container), the dashboard script may not be able to open a browser; open the generated HTML manually using the printed path.

---

If you'd like, I can also:

- Add `urllib.parse.quote_plus` URL encoding to both scripts,
- Add a lightweight `requirements.txt`, or
- Add unit tests / a smoke test for the fetcher.

Tell me which and I'll implement it.